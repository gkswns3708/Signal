{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "with open('./annotation/annotations/captions_train2014.json') as f:\n",
    "    json_array = json.load(f)\n",
    "    print(json_array.keys())\n",
    "    print(len(json_array['images']))\n",
    "    print(sorted(json_array['images'], key=lambda x : x['id'])[0])\n",
    "    print(sorted(json_array['annotations'], key=lambda x : x['image_id'])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, annotations_path, processor):\n",
    "        # annotations = dict_keys(['info', 'images', 'licenses', 'annotations'])\n",
    "        self.mode = os.path.splitext(os.path.basename(annotations_path))[0].split('_')[-1]\n",
    "        self.json_array = json.load(open(annotations_path))\n",
    "        self.dataset = {\n",
    "            'image_paths' : [],\n",
    "            'captions' : []\n",
    "        }\n",
    "        # sort forr indexing\n",
    "        for image_json, annotation_json in zip(sorted(self.json_array['images'], key=lambda x : x['id']), sorted(self.json_array['annotations'], key=lambda x : x['image_id'])):\n",
    "            # image_paths = Path(f'{self.mode}/{self.mode}' + image_json['file_name'])\n",
    "            # captions = annotation_json['captions']\n",
    "            \n",
    "            # add image_paths, caption for indexing\n",
    "            self.dataset['image_paths'].append(Path(f'{self.mode}/{self.mode}/' + image_json['file_name']))\n",
    "            self.dataset['captions'].append(annotation_json['caption']) \n",
    "        \n",
    "        # prepare processor for Natural Language Preprocessing\n",
    "        self.processor = processor\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset['image_paths'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.dataset['image_paths'][idx])\n",
    "        caption = self.dataset['captions'][idx]\n",
    "        # 사용할 processor가 batch 단위를 생각하고 encoding을 하기 때문에 squeeze()를 해야함.\n",
    "        encoding = self.processor(images=image, text=caption, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        encoding = {k : v.squeeze() for k, v in encoding.items()}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset('./annotation/annotations/captions_train2014.json', processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_total_loss = math.inf\n",
    "for epoch in range(50):\n",
    "    print(\"Epoch : \", epoch)\n",
    "    total_loss = 0\n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "            \n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                pixel_values = pixel_values,\n",
    "                                labels=input_ids)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            pbar.set_postfix(loss = loss.item())\n",
    "            pbar.update(1)\n",
    "            total_loss += loss.item()    \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    if min_total_loss > total_loss:\n",
    "        torch.save(model.state_dict(), f\"BLIP_trainin_ver-{epoch}-{total_loss:.2f}.pt\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
